https://www.bilibili.com/video/BV1RD4y137Nr/?spm_id_from=333.337.search-card.all.click&vd_source=a8fbac47f77065f88b37df80049d796b
Prometheus结合Grafana
三点半的时候，线上的cpu就报警
cpu使用率飙高，接口失败率也上去了
判断是非常迅速的，大概率跟前一天的发布有关系
所以马上进行回滚
非常核心的基础服务，差不多有20个节点
真个服务在崩溃的过程中，并没有完全的宕机
失败率达到70%
这5分钟的时间，我们进行回滚，失败率从0%到70%再到0%，这整个过程10分钟，这就是当时整个的处理过程
关键点
    cpu飙高
    这个集群是逐渐崩溃的
    cpu飙高大概率是在做一些计算密集型的运算
    整个服务节点是一个一个挂掉，大概是应用节点的问题，因为如果是一些共享资源，比如是数据库挂掉了，那整个服务就直接宕机了
    所以排查思路重点是放在应用节点上面，再结合cpu使用率飙高，这个服务也没有太多计算密集型的业务
    这个就是内存里面在做垃圾回收，大量的算法要去消耗cpu资源
然后看了一下整个集群jvm相关的监控，发现它确实发生了full GC，而且每次full gc，它的卡顿时间已经持续了5，6秒钟
监控底层
    java提供的JMX还有探针这些采集的技术
    监控用grafana去做数据展示
发生full gc的时间和整个服务失败率的飙升是一致的
所以初步判断 full gc 导致cpu使用率飙高，然后导致整个服务崩溃
为什么会发生这种事情？
    限流、监控都是非常完善的
为什么知道full gc 失败率飙高了之后，才意识到事情的严重性
前一天发布的代码业务
    提供一个rpc接口，根据上游传递来的参数，去写入底层的一张表
    起了一个异步的线程池，核心线程数1个，队列设置了100，最大线程5，拒绝策略是调用线程去执行这个线程任务
    这个任务里面有一个关键的sql，就是去批量插入一张表的数据
排查的时候发现，这个sql的耗时非常大，batch insert之前做了一些业务逻辑，导致这个insert的这个行数据非常的大，有上千行
正常的sql执行时间可能也就几毫秒，但是这个sql执行了有将近三四秒钟

梳理
    batch insert导致插入的行太多，导致慢sql
    慢sql导致这个线程池一直阻塞在这里，线程池里面的队列任务一直在堆积，在这个队列的每一个任务，都指向了一个大对象
    最开始还有一些minor gc，但后面逐渐堆积得太多，这个队列可能太长了，导致老年代被占满，触发了full gc
    然后服务就开始处于这种假死状态
    系统赔了限流，为什么没有限住
    前一天发布的，为什么第二天下午才报出来这个问题
    qps一天时间内都还是比较稳定的，特点是上午的参数比较小，下午的参数比较大，所以这里通过tps限流是限不住的
    tps没有明显的增加，只是说下午的参数，它的长度会大一些
    为什么上游的熔断也没有起效果呢
        内部启用的异步线程池，所以对它来说RT也没有明显的增加，上游的熔断也被绕过了
    所以上午业务在正常运行，这整个批量写入的过程，整个系统还抗得住
    下午调用量大了之后，整个系统其实从一点多的时候，已经开始minor gc了，已经比较频繁了
    因为线程池队列的长度没猜错的话，在逐渐递增的过程中，增加到其中一个临界点，发现老年代的空间已经不够用了，所以所就触发了full gc
    从minor gc 到full gc，中间刚好两个小时的时间

解决方案
    1.加慢sql的实时监控
        目前有sql报错的监控，但是没有慢sql的监控，如果有这个监控，就可以提前2个小时，发现这个问题
    2.使用线程池的时候，不仅要保证同时并行的线程数不能太多，cpu飙高，也要考虑到整个它的一个引用的对象，是不是有些大对象，这些风险都是要考虑到的
        java线程池，为什么要设计阻塞队列，一方面它是为了削峰，避免频繁的打到这个最大线程，然后又频繁的去销毁，这个开销比较大
        这个队列就在中间做了一些缓冲，但是这个空间你就要注意了，有没有一些大对象的引用
    3.去写数据的时候，如果能提前感知到，我们这个参数可能非常大，那代码可能也不会这样写，所以说对入参还是不太熟悉